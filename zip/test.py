import json

import pandas as pd

# Load the dataframe from train_submission.csv
df = pd.read_csv("train_submission.csv")

# Display statistics for the 'label' column
print(df["Label"].describe())
print(df["Label"].value_counts())

# Print the list of different labels
print(df["Label"].unique())

# Load the ISO 639-1 to ISO 639-3 correspondence JSON file
with open("ISO_639-1_ISO_639-3_correspondance.json", "r") as file:
    iso_data = json.load(file)

# Extract the list of ISO 639-3 codes from the JSON data
iso_639_3_codes = iso_data.values()

# List of labels to check
labels_to_check = [
    "arg",
    "lat",
    "myv",
    "tbz",
    "tsn",
    "swc",
    "kea",
    "amh",
    "ltz",
    "sot",
    "kac",
    "swe",
    "als",
    "ksh",
    "rap",
    "mon",
    "lim",
    "nde",
    "seh",
    "sat",
    "bar",
    "pus",
    "lhu",
    "vec",
    "sqi",
    "rop",
    "ber",
    "nds",
    "lin",
    "ukr",
    "lue",
    "mai",
    "hrv",
    "dan",
    "csy",
    "lao",
    "urd",
    "gug",
    "sag",
    "zlm",
    "mad",
    "lfn",
    "qvi",
    "mzh",
    "pam",
    "ewe",
    "sna",
    "khm",
    "wln",
    "bcl",
    "vol",
    "ori",
    "kos",
    "yid",
    "zsm",
    "pis",
    "kin",
    "gla",
    "mps",
    "zai",
    "xmf",
    "lus",
    "kmr",
    "nav",
    "ikk",
    "nap",
    "dzo",
    "mar",
    "que",
    "hmo",
    "che",
    "ckb",
    "iku",
    "kat",
    "srp",
    "bew",
    "kpg",
    "hin",
    "mau",
    "vie",
    "slk",
    "gym",
    "tca",
    "ssw",
    "lvs",
    "ory",
    "guj",
    "wol",
    "cjk",
    "zea",
    "aze",
    "gle",
    "npi",
    "ahk",
    "fry",
    "dyu",
    "tgk",
    "ajp",
    "tgl",
    "eng",
    "quy",
    "bos",
    "cat",
    "smo",
    "swh",
    "glv",
    "uzn",
    "kek",
    "pol",
    "bem",
    "heb",
    "pnb",
    "bul",
    "glk",
    "xav",
    "mkd",
    "pls",
    "snd",
    "tdt",
    "arn",
    "mco",
    "fur",
    "bih",
    "tha",
    "kir",
    "mlg",
    "sun",
    "hus",
    "yao",
    "xho",
    "tat",
    "oss",
    "bpy",
    "mos",
    "prs",
    "mhr",
    "fil",
    "srm",
    "glg",
    "hsb",
    "meu",
    "lug",
    "tok",
    "jam",
    "hbo",
    "mlt",
    "nep",
    "ton",
    "cak",
    "bsb",
    "fas",
    "arb",
    "tur",
    "ron",
    "tah",
    "lmo",
    "gom",
    "kan",
    "tlh",
    "spa",
    "ces",
    "guc",
    "ile",
    "sme",
    "fao",
    "cuk",
    "hau",
    "nyu",
    "tzo",
    "vep",
    "bis",
    "poh",
    "ary",
    "hrx",
    "mal",
    "kom",
    "csb",
    "kjh",
    "rug",
    "uzb",
    "pap",
    "fij",
    "ven",
    "jpn",
    "yue",
    "top",
    "udm",
    "nya",
    "lua",
    "zul",
    "ayr",
    "ban",
    "uig",
    "cmn",
    "grc",
    "fon",
    "kal",
    "ext",
    "est",
    "ote",
    "cos",
    "kik",
    "ach",
    "crh",
    "ngu",
    "nno",
    "tir",
    "hye",
    "mwl",
    "tam",
    "hbs",
    "jav",
    "umb",
    "kur",
    "epo",
    "ctu",
    "kaz",
    "aoj",
    "yor",
    "pms",
    "gor",
    "div",
    "zho",
    "hil",
    "krc",
    "twi",
    "hun",
    "azb",
    "bzj",
    "run",
    "cbk",
    "bod",
    "quw",
    "alt",
    "cym",
    "nbl",
    "sin",
    "nob",
    "aym",
    "yap",
    "swa",
    "kaa",
    "frr",
    "apc",
    "luo",
    "ell",
    "cab",
    "nnb",
    "sah",
    "jbo",
    "bak",
    "mam",
    "szl",
    "ekk",
    "afr",
    "diq",
    "san",
    "ixl",
    "tyv",
    "djk",
    "kab",
    "ace",
    "tuk",
    "mgh",
    "pcd",
    "wal",
    "kon",
    "new",
    "dtp",
    "naq",
    "vls",
    "som",
    "bqc",
    "pes",
    "wbm",
    "hmn",
    "chv",
    "sco",
    "gcf",
    "ksd",
    "eus",
    "mri",
    "aln",
    "abk",
    "eml",
    "grn",
    "fra",
    "acm",
    "ido",
    "afb",
    "lzh",
    "tso",
    "bik",
    "war",
    "bam",
    "msa",
    "suz",
    "bel",
    "min",
    "roh",
    "slv",
    "ina",
    "bre",
    "rue",
    "quh",
    "nch",
    "kor",
    "arz",
    "tpi",
    "oci",
    "pan",
    "ibo",
    "lij",
    "quc",
    "gsw",
    "knv",
    "mya",
    "asm",
    "srd",
    "kmb",
    "ilo",
    "aka",
    "ast",
    "toj",
    "kjb",
    "hat",
    "plt",
    "nso",
    "acr",
    "kam",
    "lit",
    "ceb",
    "rmy",
    "pon",
    "kbd",
    "ful",
    "bjn",
    "ita",
    "mzn",
    "scn",
    "tls",
    "isl",
    "pcm",
    "hui",
    "pag",
    "chk",
    "nld",
    "orm",
    "teo",
    "por",
    "wuu",
    "ara",
    "azj",
    "yom",
    "tum",
    "deu",
    "sgs",
    "ind",
    "hif",
    "kbp",
    "gaa",
    "nan",
    "hne",
    "fin",
    "ncj",
    "pfl",
    "srn",
    "tuc",
    "miq",
    "ndo",
    "haw",
    "hyw",
    "kua",
    "nor",
    "hnj",
    "ada",
    "qub",
    "ngl",
    "enm",
    "mah",
    "niu",
    "crs",
    "iba",
    "tvl",
    "ksw",
    "pau",
    "quz",
    "gcr",
    "gil",
    "toi",
]

# Initialize counters for found and not found labels
found_count = 0
not_found_count = 0

# Check if the labels in the list are present in the ISO 639-3 codes
for label in labels_to_check:
    if label in iso_639_3_codes:
        found_count += 1
    else:
        not_found_count += 1
        print(f"Label '{label}' not found in ISO 639-3 codes")

# Print the counts of found and not found labels
print(f"Found labels: {found_count}")
print(f"Not found labels: {not_found_count}")

# Print dupplicated in ID
print(df[df.duplicated(subset=["ID"], keep=False)])
